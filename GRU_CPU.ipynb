{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73358686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD THE LIBRARIES YOU'LL NEED\n",
    "import os\n",
    "import sys\n",
    "from re import M\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from pandas.tseries.offsets import Micro\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
    "    f1_score, recall_score, precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# import fasttext\n",
    "import gensim\n",
    "import torch\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6863c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "global fast_model \n",
    "global allwords\n",
    "print(torch.__version__)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dbc15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vishak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "WORD_DICT = None\n",
    "nltk.download('stopwords')\n",
    "TOKENIZER = RegexpTokenizer(r'\\w+')\n",
    "NEG_INDICATORS = set(['but', 'if', 'or', 'because', 'until', 'against', 'between',\n",
    "                      'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "                      'such', 'no', 'nor', 'not', 'only', 'so', 'than', 'too', 'very',\n",
    "                      't', 'can', 'will', 'just', 'don', 'should', \"should've\", 'now',\n",
    "                      'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn',\n",
    "                      'haven', 'isn', 'mightn', 'mustn', 'needn', 'shan', 'shouldn',\n",
    "                      'wasn', 'weren', 'won', 'wouldn'])\n",
    "STOPWORDS = set(stopwords.words('english')).difference(NEG_INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56c3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_vectors(text: str):\n",
    "    if text in GLOVE_MODEL:\n",
    "        return GLOVE_MODEL[text]\n",
    "    elif text == \"\":\n",
    "        return np.zeros((1, 50))\n",
    "    else:\n",
    "        return np.random.randn(1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87fc7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_vectors_fast(text: str):\n",
    "    global fast_model,allwords\n",
    "    #print(text)\n",
    "    if text in fast_model.vocab:\n",
    "        #ind=allwords.index(text)\n",
    "        return fast_model[text].reshape((1,100))\n",
    "    elif text == \"\":\n",
    "        return np.zeros((1, 100))\n",
    "    else:\n",
    "        return np.random.randn(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793d7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(text: List[str]):\n",
    "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary)\n",
    "\n",
    "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like:\n",
    "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to\n",
    "    # encode the words into integers\n",
    "\n",
    "    # return encoded examples\n",
    "    vector_list = map(encoded_vectors, text)\n",
    "    vector_list = map(lambda x: np.reshape(x, (1, 50)), vector_list)\n",
    "    sent_encoding = np.column_stack(tuple(vector_list))\n",
    "\n",
    "    # print(sent_encoding)\n",
    "    # print(sent_encoding.shape)\n",
    "\n",
    "    return sent_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f03245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_fast(text: List[str]):\n",
    "    # This function will be used to encode the reviews using a dictionary(created using corpus vocabulary)\n",
    "\n",
    "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like:\n",
    "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to\n",
    "    # encode the words into integers\n",
    "\n",
    "    # return encoded examples\n",
    "    #print(\"FAST\")\n",
    "    vector_list = map(encoded_vectors_fast, text)\n",
    "    vector_list = map(lambda x: np.reshape(x, (1, 100)), vector_list)\n",
    "    #print(np.column_stack(tuple(vector_list)).shape)\n",
    "    sent_encoding = np.expand_dims(np.squeeze(tuple(vector_list)),0)\n",
    "  # print(sent_encoding.shape)\n",
    "\n",
    "    # print(sent_encoding)\n",
    "    # print(sent_encoding.shape)\n",
    "\n",
    "    return sent_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692cde92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dict(reviews, size: int = 2000):\n",
    "    corp = list(itertools.chain.from_iterable(list(reviews)))\n",
    "    counter = Counter(corp)\n",
    "    most_common_set = counter.most_common()[:min(size, len(counter))]\n",
    "    print(\"Most common set\", len(most_common_set))\n",
    "    global WORD_DICT\n",
    "    WORD_DICT = dict()\n",
    "    for i, (w, c) in enumerate(most_common_set):\n",
    "        WORD_DICT[w] = i\n",
    "    print(\"word_dict\", len(WORD_DICT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7595370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data2(text: List[str]):\n",
    "    global WORD_DICT\n",
    "    if WORD_DICT is None:\n",
    "        raise ValueError(\"WORD_DICT not defined\")\n",
    "    result = np.zeros(len(WORD_DICT))\n",
    "    for w in text:\n",
    "        if w in WORD_DICT:\n",
    "            result[WORD_DICT[w]] = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867b2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return str.lower(text)\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # return the reviews after removing punctuations\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "649b618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: List[str]):\n",
    "    # return the reviews after removing the stopwords\n",
    "    return list(filter(lambda x: x not in STOPWORDS, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f407c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tokenization(text):\n",
    "    # return the reviews after performing tokenization\n",
    "    return TOKENIZER.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49002805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_padding(data: List[str], pad_len: int = 40):\n",
    "    # return the reviews after padding the reviews to maximum length\n",
    "    return data + (pad_len - len(data)) * [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf83769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame, encoder: str = 'glove'):\n",
    "    # make all the following function calls on your data\n",
    "    # EXAMPLE:->\n",
    "    '''\n",
    "    review = data[\"reviews\"]\n",
    "    review = convert_to_lower(review)\n",
    "    review = remove_punctuation(review)\n",
    "    review = remove_stopwords(review)\n",
    "    review = perform_tokenization(review)\n",
    "    review = encode_data(review)\n",
    "    review = perform_padding(review)\n",
    "    '''\n",
    "\n",
    "    reviews = data['reviews']\n",
    "    reviews = reviews.apply(convert_to_lower)\n",
    "    reviews = reviews.apply(perform_tokenization)\n",
    "    # print(reviews.head())\n",
    "    reviews = reviews.apply(remove_stopwords)\n",
    "    # print(\"max len\", reviews.apply(len).max())\n",
    "    if encoder == 'glove':\n",
    "        reviews = reviews.apply(perform_padding)\n",
    "        reviews = reviews.apply(encode_data)\n",
    "    elif encoder == 'bow':\n",
    "        global WORD_DICT\n",
    "        if WORD_DICT is None:\n",
    "            create_word_dict(reviews)\n",
    "            print(\"Created Word Dict....\")\n",
    "        reviews = map(encode_data2, list(reviews))\n",
    "    elif encoder == 'fast':\n",
    "        reviews = reviews.apply(perform_padding)\n",
    "        reviews = reviews.apply(encode_data_fast)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid encoder value\")\n",
    "    # reviews =\n",
    "    # print(reviews.loc[0])\n",
    "    # print(tuple(reviews)[0])\n",
    "    reviews = np.row_stack(tuple(reviews))\n",
    "    # print(reviews.shape)\n",
    "    # print(reviews.head())\n",
    "\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f16cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(x):\n",
    "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
    "    x_exp = torch.exp(x)\n",
    "    return x_exp / x_exp.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d47c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, reviews, ratings):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.reviews = torch.from_numpy(reviews.astype(np.float32))\n",
    "        self.ratings = torch.from_numpy(ratings)\n",
    "        print(\"REVIEWS\", reviews.shape)\n",
    "        self.inp_sz = reviews.shape[1]\n",
    "\n",
    "    def build_nn(self, weighted: bool = False):\n",
    "        # add the input and output layer here; you can use either tensorflow or pytorch\n",
    "\n",
    "        #weightSize = 2048\n",
    "        #self.hidden1 = nn.Linear(self.inp_sz,weightSize)\n",
    "        #self.hidden2 = nn.Linear(weightSize,weightSize)\n",
    "        # self.hidden3 = nn.Linear(weightSize,weightSize)\n",
    "        #self.fc1 = nn.Linear(weightSize, 5)\n",
    "        #self.lrelu = nn.LeakyReLU()\n",
    "        '''\n",
    "        self.rnn = torch.nn.LSTM(input_size = 100, \n",
    "                              hidden_size= 200, \n",
    "                              num_layers = 3, \n",
    "                              batch_first=True)\n",
    "        '''\n",
    "        \n",
    "        self.rnn = torch.nn.GRU(input_size = 100, \n",
    "                              hidden_size= 200, \n",
    "                              num_layers = 3, \n",
    "                              batch_first=True)\n",
    "                              \n",
    "        '''\n",
    "        self.rnn = torch.nn.GRU(input_size = 100, \n",
    "                              hidden_size= 200, \n",
    "                              num_layers = 3, \n",
    "                              batch_first=True,\n",
    "                              bidirectional=True)\n",
    "        '''\n",
    "        self.out_layer = torch.nn.Linear(200, 5)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        if weighted:\n",
    "            self.loss = nn.CrossEntropyLoss(weight=torch.tensor([4059, 2265, 3612, 6871, 33193.]).reciprocal())#.cuda(0)\n",
    "        else:\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_nn(self, batch_size, epochs):\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # write the training loop here; you can use either tensorflow or pytorch\n",
    "        # print validation accuracy\n",
    "        print(\"reviews shape\", self.reviews.shape)\n",
    "        print(\"ratings shape\", self.ratings.shape)\n",
    "        print(\"reviews shape\", self.reviews[:500].shape)\n",
    "        print(\"ratings shape\", self.ratings[:500].shape)\n",
    "\n",
    "        train_ds = TensorDataset(self.reviews[:45000], self.ratings[:45000])\n",
    "        val_ds = TensorDataset(self.reviews[45000:], self.ratings[45000:])\n",
    "        # train_tensor_ds = train_tensor_ds[:40000]\n",
    "        print(\"validation\", len(val_ds))\n",
    "        # print(val_ds[1])\n",
    "        print(\"train\", len(train_ds))\n",
    "\n",
    "\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "        if use_cuda : \n",
    "          self = self.cuda()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "        #optim.SGD([\n",
    "        #        {'params': model.base.parameters()},\n",
    "         #       {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "          #  ], lr=1e-2, momentum=0.9)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch:\",epoch)\n",
    "            for x, y  in train_dl:\n",
    "                if use_cuda : \n",
    "                  x,y = x,y\n",
    "                error = self.loss(self.forward(x), y)\n",
    "                optimizer.zero_grad()\n",
    "                error.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                #train_ds_0 , val_ds_0 ,train_ds_1, val_ds_1 = train_ds[:][0].cuda() , val_ds[:][0].cuda() , train_ds[:][1].cuda() , val_ds[:][1].cuda()\n",
    "            #else:\n",
    "                train_ds_0 , val_ds_0 ,train_ds_1, val_ds_1 = train_ds[:][0] , val_ds[:][0],  train_ds[:][1] , val_ds[:][1]\n",
    "\n",
    "            #y_train = self.forward(train_ds_0)\n",
    "            #loss_train = self.loss(y_train, train_ds_1)\n",
    "            #acc_train = accuracy_score(train_ds[:][1].numpy(), y_train.cpu().argmax(dim=1, keepdim=False).numpy())\n",
    "\n",
    "            print(\"validating...\")\n",
    "            y_val = self.forward(val_ds_0)\n",
    "            loss_val = self.loss(y_val, val_ds_1)\n",
    "            acc_val = accuracy_score(val_ds[:][1].numpy(), y_val.cpu().argmax(dim=1, keepdim=False).numpy())\n",
    "\n",
    "            print(\"Epoch: {} \"\n",
    "                  \"ValLoss: {}, ValAcc: {}\".format(epoch + 1, loss_val, acc_val))\n",
    "\n",
    "        print(\"\\nTraining Complete......\")\n",
    "\n",
    "    def forward(self, reviews):\n",
    "        #out = reviews\n",
    "        #out = torch.relu(self.hidden1(out))\n",
    "        #out =  torch.relu(self.hidden2(out))\n",
    "        # out = F.relu(self.hidden3(out))\n",
    "        #return softmax_activation((self.fc1(out)))\n",
    "        #print(reviews.shape)\n",
    "        out, _ = self.rnn(reviews)\n",
    "        x = self.out_layer(out[:,-1,:])\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def predict(self, reviews):\n",
    "        reviews_t = torch.from_numpy(reviews.astype(np.float32))\n",
    "        # return a list containing all the ratings predicted by the trained model\n",
    "        #self = self.cpu()\n",
    "        #print\n",
    "        return self.forward(reviews_t).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "667fbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(x: int):\n",
    "    if x == 1:\n",
    "        return np.array([[1, 0, 0, 0, 0]])\n",
    "    if x == 2:\n",
    "        return np.array([[0, 1, 0, 0, 0]])\n",
    "    if x == 3:\n",
    "        return np.array([[0, 0, 1, 0, 0]])\n",
    "    if x == 4:\n",
    "        return np.array([[0, 0, 0, 1, 0]])\n",
    "    if x == 5:\n",
    "        return np.array([[0, 0, 0, 0, 1]])\n",
    "    else:\n",
    "        raise ValueError(\"UnIdentified Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0e06ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_col(labels: pd.DataFrame):\n",
    "    labels = labels['ratings']\n",
    "    labels = labels.apply(encode_label)\n",
    "    labels = np.row_stack(tuple(labels))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c5b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(pred_probs: np.ndarray, ground_truth):\n",
    "    pred_labels = pred_probs.argmax(axis=1)\n",
    "\n",
    "    print(\"\\nEvaluation Metrics\")\n",
    "    print(\"Accuracy : \", accuracy_score(ground_truth, pred_labels))\n",
    "    print(\"Precision: \", precision_score(ground_truth, pred_labels, average='micro'))\n",
    "    print(\"Recall   : \", recall_score(ground_truth, pred_labels, average='micro'))\n",
    "    print(\"F1-Score : \", f1_score(ground_truth, pred_labels, average='micro'))\n",
    "    print(\"Confusion Matrix\")\n",
    "    conf_df = pd.DataFrame(confusion_matrix(ground_truth, pred_labels),\n",
    "                           index=[\"true_1\", \"true_2\", \"true_3\", \"true_4\", \"true_5\"],\n",
    "                           columns=[\"pred_1\", \"pred_2\", \"pred_3\", \"pred_4\", \"pred_5\"])\n",
    "    print(conf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b85096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_file, test_file):\n",
    "    batch_size, epochs = (16, 20)\n",
    "\n",
    "    print(\"11\")\n",
    "    global fast_model\n",
    "    fast_model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "    '''\n",
    "    print(\"22\")\n",
    "    print(type(fast_model_1.wv.vocab))\n",
    "    print(type(list(fast_model_1.wv.vocab.values())))\n",
    "   ## PCA fast_model = \n",
    "    allwordvec=[]\n",
    "    global allwords\n",
    "    allwords=list(fast_model_1.wv.vocab.keys())\n",
    "    for word in allwords:\n",
    "      allwordvec.append(fast_model_1[word])\n",
    "    #pca = PCA(n_components=50, svd_solver='full')\n",
    "    print(len(allwordvec))\n",
    "    print(len(allwordvec[0]))\n",
    "    pca.fit(np.asarray(allwordvec).transpose())\n",
    "    fast_model = pca.components_.transpose()\n",
    "    print(fast_model.shape)\n",
    "    fast_model_1 = 0\n",
    "    '''\n",
    "\n",
    "     \n",
    "    # Load training dataframe\n",
    "    #train_data: pd.DataFrame = pd.read_csv(train_file, header=0, index_col=0)\n",
    "    #train_data = train_data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    test_data: pd.DataFrame = pd.read_csv(test_file, header=0, index_col=0)\n",
    "\n",
    "    #print(\"Train Data distribution\")\n",
    "    #print(train_data.groupby(['ratings']).agg(['count']))\n",
    "    print(\"Test Data distribution\")\n",
    "    print(test_data.groupby(['ratings']).agg(['count']))\n",
    "\n",
    "    #print('Train Dataframe....')\n",
    "    #print(train_data.head(), \"\\n\")\n",
    "    print('Test Dataframe....')\n",
    "    print(test_data.head(), \"\\n\")\n",
    "\n",
    "    # Embedding Supported: fast , glove , bow\n",
    "    #train_reviews = preprocess_data(train_data, encoder='fast')  # \"gov\"\n",
    "    test_reviews = preprocess_data(test_data, encoder='fast')\n",
    "    print(\"090\")\n",
    "\n",
    "    #train_ratings = train_data['ratings'].to_numpy().reshape((-1,)) - 1\n",
    "    test_ratings = test_data['ratings'].to_numpy().reshape((-1,)) - 1\n",
    "\n",
    "    print(\"123\")\n",
    "    \n",
    "    '''\n",
    "    model = NeuralNet(train_reviews, train_ratings)\n",
    "    #model=model.cuda()\n",
    "    print(\"456\")\n",
    "    model.build_nn(weighted=True)\n",
    "    print(\"789\")\n",
    "    model.train_nn(batch_size, epochs)\n",
    "    print(\"101112\")\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    model = torch.load('saved_model')\n",
    "    #torch.save(model, \"saved_model\")\n",
    "    pred_probs: np.ndarray = model.predict(test_reviews)\n",
    "\n",
    "    \n",
    "    evaluate_results(pred_probs, test_ratings)\n",
    "\n",
    "    prediction_df = pd.DataFrame(pred_probs,\n",
    "                                 columns=[\"prob_1\", \"prob_2\", \"prob_3\", \"prob_4\", \"prob_5\"])\n",
    "    prediction_df['predicted_Class'] = pred_probs.argmax(axis=1) + 1\n",
    "    # prediction_df[\"true_ratings\"] = test_ratings +1\n",
    "    print(\"\\nPredictions\\n\")\n",
    "    print(prediction_df)\n",
    "    return prediction_df, pred_probs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2434cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, _ = main(\"train.csv\",\"gold_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "039fbdd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name is : This movie is awful\n",
      "This movie is awful\n",
      "11\n",
      "Test Data distribution\n",
      "        reviews\n",
      "          count\n",
      "ratings        \n",
      "1          1271\n",
      "2           630\n",
      "3           913\n",
      "4          1404\n",
      "5          5784\n",
      "Test Dataframe....\n",
      "                                             reviews  ratings\n",
      "0  Doesn't work at ALL. Don't waste your money or...        1\n",
      "1  What crap.  Would need a lot more power to do ...        1\n",
      "2  Has no suction and didn't work. Not worth trying.        1\n",
      "3  That is definitely a trash. Unable to clean an...        1\n",
      "4  Didn't even worked on cleaning the ears at all...        1 \n",
      "\n",
      "090\n",
      "123\n",
      "\n",
      "Evaluation Metrics\n",
      "Accuracy :  0.6261747650469907\n",
      "Precision:  0.6261747650469907\n",
      "Recall   :  0.6261747650469907\n",
      "F1-Score :  0.6261747650469907\n",
      "Confusion Matrix\n",
      "        pred_1  pred_2  pred_3  pred_4  pred_5\n",
      "true_1     781     309     111      20      50\n",
      "true_2     187     228     145      39      31\n",
      "true_3     101     207     413     134      58\n",
      "true_4      34      96     351     516     407\n",
      "true_5     129      87     276     967    4325\n",
      "\n",
      "Predictions\n",
      "\n",
      "         prob_1    prob_2    prob_3    prob_4    prob_5  predicted_Class\n",
      "0     -0.043010 -3.302095 -5.570251 -7.320211 -7.105597                1\n",
      "1     -0.240854 -1.800500 -3.377649 -5.181710 -4.701504                1\n",
      "2     -0.195349 -1.912591 -3.663965 -6.044734 -6.345470                1\n",
      "3     -0.290735 -1.782240 -2.981926 -4.336489 -3.899682                1\n",
      "4     -0.372242 -1.312954 -3.373080 -5.410414 -5.797619                1\n",
      "...         ...       ...       ...       ...       ...              ...\n",
      "9997  -6.492541 -6.853928 -3.779912 -0.414315 -1.158960                4\n",
      "9998  -4.186713 -6.411477 -4.881717 -1.790053 -0.212419                5\n",
      "9999  -0.059128 -2.970888 -5.350562 -7.256110 -7.258113                1\n",
      "10000 -0.031510 -3.754524 -5.618547 -6.643422 -5.923856                1\n",
      "10001 -0.031510 -3.754524 -5.618547 -6.643422 -5.923856                1\n",
      "\n",
      "[10002 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "root=tk.Tk()\n",
    " \n",
    "# setting the windows size\n",
    "root.geometry(\"600x400\")\n",
    "\n",
    "# declaring string variable\n",
    "# for storing name and password\n",
    "name_var=tk.StringVar()\n",
    "#passw_var=tk.StringVar()\n",
    "rating_var=tk.StringVar()\n",
    "global sentiment_test\n",
    "global rating_test\n",
    "\n",
    "# defining a function that will\n",
    "# get the name and password and\n",
    "# print them on the screen\n",
    "def submit(rating_entry):\n",
    "    \n",
    "    global sentiment\n",
    "    global rating\n",
    " \n",
    "    name=name_var.get()\n",
    "    #password=passw_var.get()\n",
    "     \n",
    "    print(\"The name is : \" + name)\n",
    "    #print(\"The password is : \" + password)\n",
    "    \n",
    "    sentiment_test = name\n",
    "    #rating_test = password\n",
    "    rating = fn(name)\n",
    "    rating_entry.insert(0, rating)\n",
    "\n",
    "    #name_var.set(\"\")\n",
    "    #passw_var.set(\"\")\n",
    "    \n",
    "# creating a label for\n",
    "# name using widget Label\n",
    "name_label = tk.Label(root, text = 'Sentiment', font=('calibre',10, 'bold'))\n",
    "# creating a entry for input\n",
    "# name using widget Entry\n",
    "name_entry = tk.Entry(root,textvariable = name_var, font=('calibre',10,'normal'))\n",
    "\n",
    "# creating a label for password\n",
    "#passw_label = tk.Label(root, text = 'Dummy rating', font = ('calibre',10,'bold'))\n",
    "  \n",
    "# creating a entry for password\n",
    "#passw_entry=tk.Entry(root, textvariable = passw_var, font = ('calibre',10,'normal'))\n",
    "\n",
    "\n",
    "# creating a label for password\n",
    "rating_label = tk.Label(root, text = 'Rating', font = ('calibre',10,'bold'))\n",
    "  \n",
    "# creating a entry for password\n",
    "rating_entry=tk.Entry(root, textvariable = rating_var, font = ('calibre',10,'normal'))\n",
    "\n",
    "\n",
    "\n",
    "# creating a button using the widget\n",
    "# Button that will call the submit function\n",
    "sub_btn=tk.Button(root,text = 'Submit', command = lambda: submit(rating_entry))\n",
    "\n",
    "# placing the label and entry in\n",
    "# the required position using grid\n",
    "# method\n",
    "name_label.grid(row=0,column=0)\n",
    "name_entry.grid(row=0,column=1)\n",
    "#passw_label.grid(row=1,column=0)\n",
    "#passw_entry.grid(row=1,column=1)\n",
    "rating_label.grid(row=2,column=0)\n",
    "rating_entry.grid(row=2,column=1)\n",
    "sub_btn.grid(row=5,column=1)\n",
    "\n",
    "  \n",
    "# performing an infinite loop\n",
    "# for the window to display\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff6ab389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def fn(sentiment):\n",
    "    print(sentiment)\n",
    "    with open('trial.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "    \n",
    "        writer.writerow([1, sentiment, \"3\"])\n",
    "    \n",
    "    predictions, _ = main(\"train.csv\",\"trial.csv\")\n",
    "    return(predictions['predicted_Class'].to_list()[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
